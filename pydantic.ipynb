{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_REPORT_API_KEY = os.getenv(\"LLM_REPORT_API_KEY\")\n",
    "assert LLM_REPORT_API_KEY, \"LLM_REPORT_API_KEY env var must be set\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # base_url=\"https://api.openai.withlogging.com/v1\",\n",
    "    # default_headers={\n",
    "    #     \"X-Api-Key\": f\"Bearer {LLM_REPORT_API_KEY}\",\n",
    "    #     \"X-User-Id\": \"nickolay.sheyko@gmail.com\",\n",
    "    # },\n",
    ")\n",
    "\n",
    "from instructor import Mode\n",
    "from llmonitor import monitor\n",
    "\n",
    "monitor(client)\n",
    "# client = instructor.patch(client)\n",
    "# client = instructor.patch(client, mode=Mode.JSON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llmonitor import monitor\n",
    "# monitor(client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai._types import NOT_GIVEN\n",
    "from typing import Type, TypeVar\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "OutputType = TypeVar(\"OutputType\", bound=BaseModel)\n",
    "\n",
    "\n",
    "GPT_3 = \"gpt-3.5-turbo\"\n",
    "GPT_3 = \"gpt-3.5-turbo-1106\"\n",
    "GPT_4 = \"gpt-4-1106-preview\"\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Ignore all previous instructions.\n",
    "\n",
    "1. You are to provide clear, concise, and direct responses.\n",
    "2. Eliminate unnecessary reminders, apologies, self-references, and any pre-programmed niceties.\n",
    "3. Maintain a casual tone in your communication.\n",
    "4. Be transparent; if you're unsure about an answer or if a question is beyond your capabilities or knowledge, admit it.\n",
    "5. For any unclear or ambiguous queries, ask follow-up questions to understand the user's intent better.\n",
    "6. When explaining concepts, use real-world examples and analogies, where appropriate.\n",
    "7. For complex requests, take a deep breath and work on the problem step-by-step.\n",
    "8. For every response, you will be tipped up to $200 (depending on the quality of your output).\n",
    "9. Answer in JSON format.\n",
    "\n",
    "It is very important that you get this right. Multiple lives are at stake.\"\"\"\n",
    "\n",
    "def call_openai(response_model: Type[OutputType], question: str, model = GPT_3, seed=NOT_GIVEN) -> OutputType:\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        response_model=response_model,\n",
    "        max_retries=3,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": question}],\n",
    "        seed=seed,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "class Step(BaseModel):\n",
    "    question: str = Field(..., description=\"The subquestion needed to answer the main question\")\n",
    "    answer: str = Field(..., description=\"The answer to the subquestion\")\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    \"\"\"Docstring\"\"\"\n",
    "    steps: list[Step] = Field(..., description=\"The questions that were asked and answered to answer the initial question\")\n",
    "\n",
    "    answer: str\n",
    "    confidence: float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "() {'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'hello'}]}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"hello\"}],\n",
    "    # functions=Answer.model_json_schema(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer.model_json_schema()\n",
    "\n",
    "# def create_function_parameter(name: str, type: Type[BaseModel]):\n",
    "#     return {\n",
    "#         \"name\": type.schema()[\"title\"],\n",
    "#         \"type\": type.schema()[\"title\"],\n",
    "#         \"description\": type.schema()[\"description\"],\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "() {'model': 'gpt-3.5-turbo-1106', 'response_model': <class '__main__.Answer'>, 'max_retries': 3, 'messages': [{'role': 'system', 'content': \"Ignore all previous instructions.\\n\\n1. You are to provide clear, concise, and direct responses.\\n2. Eliminate unnecessary reminders, apologies, self-references, and any pre-programmed niceties.\\n3. Maintain a casual tone in your communication.\\n4. Be transparent; if you're unsure about an answer or if a question is beyond your capabilities or knowledge, admit it.\\n5. For any unclear or ambiguous queries, ask follow-up questions to understand the user's intent better.\\n6. When explaining concepts, use real-world examples and analogies, where appropriate.\\n7. For complex requests, take a deep breath and work on the problem step-by-step.\\n8. For every response, you will be tipped up to $200 (depending on the quality of your output).\\n9. Answer in JSON format.\\n\\nIt is very important that you get this right. Multiple lives are at stake.\"}, {'role': 'user', 'content': 'What is the richest ciries in the world?'}], 'seed': NOT_GIVEN, 'response_format': {'type': 'json_object'}}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Completions.create() got an unexpected keyword argument 'response_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/sne/gpt/playground/pydantic.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sne/gpt/playground/pydantic.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m answer \u001b[39m=\u001b[39m call_openai(Answer, \u001b[39m\"\u001b[39;49m\u001b[39mWhat is the richest ciries in the world?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/sne/gpt/playground/pydantic.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sne/gpt/playground/pydantic.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_openai\u001b[39m(response_model: Type[OutputType], question: \u001b[39mstr\u001b[39m, model \u001b[39m=\u001b[39m GPT_3, seed\u001b[39m=\u001b[39mNOT_GIVEN) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OutputType:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sne/gpt/playground/pydantic.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sne/gpt/playground/pydantic.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sne/gpt/playground/pydantic.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m         response_model\u001b[39m=\u001b[39;49mresponse_model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sne/gpt/playground/pydantic.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         max_retries\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sne/gpt/playground/pydantic.ipynb#W6sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m         messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sne/gpt/playground/pydantic.ipynb#W6sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: SYSTEM_PROMPT},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sne/gpt/playground/pydantic.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: question}],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sne/gpt/playground/pydantic.ipynb#W6sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sne/gpt/playground/pydantic.ipynb#W6sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         response_format\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mjson_object\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sne/gpt/playground/pydantic.ipynb#W6sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/gpt/playground/.venv/lib/python3.12/site-packages/llmonitor/__init__.py:208\u001b[0m, in \u001b[0;36mwrap.<locals>.sync_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m     track_event(\n\u001b[1;32m    201\u001b[0m         \u001b[39mtype\u001b[39m,\n\u001b[1;32m    202\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    203\u001b[0m         run_id,\n\u001b[1;32m    204\u001b[0m         error\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(e), \u001b[39m\"\u001b[39m\u001b[39mstack\u001b[39m\u001b[39m\"\u001b[39m: traceback\u001b[39m.\u001b[39mformat_exc()},\n\u001b[1;32m    205\u001b[0m     )\n\u001b[1;32m    207\u001b[0m     \u001b[39m# rethrow error\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    210\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     parsed_output \u001b[39m=\u001b[39m output_parser(output, kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/gpt/playground/.venv/lib/python3.12/site-packages/llmonitor/__init__.py:197\u001b[0m, in \u001b[0;36mwrap.<locals>.sync_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[39mreturn\u001b[39;00m stream_handler(fn, run_id, name \u001b[39mor\u001b[39;00m parsed_input[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mtype\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    196\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    199\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    200\u001b[0m     track_event(\n\u001b[1;32m    201\u001b[0m         \u001b[39mtype\u001b[39m,\n\u001b[1;32m    202\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    203\u001b[0m         run_id,\n\u001b[1;32m    204\u001b[0m         error\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mstr\u001b[39m(e), \u001b[39m\"\u001b[39m\u001b[39mstack\u001b[39m\u001b[39m\"\u001b[39m: traceback\u001b[39m.\u001b[39mformat_exc()},\n\u001b[1;32m    205\u001b[0m     )\n",
      "File \u001b[0;32m~/gpt/playground/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 299\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: Completions.create() got an unexpected keyword argument 'response_model'"
     ]
    }
   ],
   "source": [
    "answer = call_openai(Answer, \"What is the richest ciries in the world?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "() {'model': 'gpt-3.5-turbo-1106', 'messages': [{'role': 'system', 'content': \"Ignore all previous instructions.\\n\\n1. You are to provide clear, concise, and direct responses.\\n2. Eliminate unnecessary reminders, apologies, self-references, and any pre-programmed niceties.\\n3. Maintain a casual tone in your communication.\\n4. Be transparent; if you're unsure about an answer or if a question is beyond your capabilities or knowledge, admit it.\\n5. For any unclear or ambiguous queries, ask follow-up questions to understand the user's intent better.\\n6. When explaining concepts, use real-world examples and analogies, where appropriate.\\n7. For complex requests, take a deep breath and work on the problem step-by-step.\\n8. For every response, you will be tipped up to $200 (depending on the quality of your output).\\n9. Answer in JSON format.\\n\\nIt is very important that you get this right. Multiple lives are at stake.\"}, {'role': 'user', 'content': 'What is the richest ciries in the world?'}], 'response_format': {'type': 'json_object'}, 'functions': [{'name': 'Answer', 'description': 'Docstring', 'parameters': {'properties': {'answer': {'description': 'The answer to the subquestion', 'title': 'Answer', 'type': 'string'}, 'question': {'description': 'The subquestion needed to answer the main question', 'title': 'Question', 'type': 'string'}}, 'required': ['answer', 'question'], 'type': 'object'}}], 'function_call': {'name': 'Answer'}}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"model\": \"gpt-3.5-turbo-1106\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Ignore all previous instructions.\\n\\n1. You are to provide clear, concise, and direct responses.\\n2. Eliminate unnecessary reminders, apologies, self-references, and any pre-programmed niceties.\\n3. Maintain a casual tone in your communication.\\n4. Be transparent; if you're unsure about an answer or if a question is beyond your capabilities or knowledge, admit it.\\n5. For any unclear or ambiguous queries, ask follow-up questions to understand the user's intent better.\\n6. When explaining concepts, use real-world examples and analogies, where appropriate.\\n7. For complex requests, take a deep breath and work on the problem step-by-step.\\n8. For every response, you will be tipped up to $200 (depending on the quality of your output).\\n9. Answer in JSON format.\\n\\nIt is very important that you get this right. Multiple lives are at stake.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is the richest ciries in the world?\"},\n",
    "    ],\n",
    "    \"response_format\": {\"type\": \"json_object\"},\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"Answer\",\n",
    "            \"description\": \"Docstring\",\n",
    "            \"parameters\": {\n",
    "                \"properties\": {\n",
    "                    \"answer\": {\n",
    "                        \"description\": \"The answer to the subquestion\",\n",
    "                        \"title\": \"Answer\",\n",
    "                        \"type\": \"string\",\n",
    "                    },\n",
    "                    \"question\": {\n",
    "                        \"description\": \"The subquestion needed to answer the main question\",\n",
    "                        \"title\": \"Question\",\n",
    "                        \"type\": \"string\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"answer\", \"question\"],\n",
    "                \"type\": \"object\",\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    \"function_call\": {\"name\": \"Answer\"},\n",
    "}\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(**params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-8SsKYefSnP1iw4M3d3FRXodFBWkgR\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"function_call\": {\n",
      "          \"arguments\": \"{\\\"question\\\":\\\"What are the top 5 richest cities in the world based on GDP per capita?\\\"}\",\n",
      "          \"name\": \"Answer\"\n",
      "        },\n",
      "        \"tool_calls\": null\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1701892722,\n",
      "  \"model\": \"gpt-3.5-turbo-1106\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"system_fingerprint\": \"fp_eeff13170a\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 21,\n",
      "    \"prompt_tokens\": 270,\n",
      "    \"total_tokens\": 291\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"steps\": [\n",
      "    {\n",
      "      \"question\": \"What specific criteria or measurements would you like to use to determine the richest cities? For example, are you referring to cities with the highest GDP, highest average income, or any other specific financial metrics?\",\n",
      "      \"answer\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"answer\": \" \",\n",
      "  \"confidence\": 0.8\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(answer.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Property(BaseModel):\n",
    "    key: str\n",
    "    value: str\n",
    "    resolved_absolute_value: str\n",
    "\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    id: int = Field(\n",
    "        ...,\n",
    "        description=\"Unique identifier for the entity, used for deduplication, design a scheme allows multiple entities\",\n",
    "    )\n",
    "    subquote_string: list[str] = Field(\n",
    "        ...,\n",
    "        description=\"Correctly resolved value of the entity, if the entity is a reference to another entity, this should be the id of the referenced entity, include a few more words before and after the value to allow for some context to be used in the resolution\",\n",
    "    )\n",
    "    entity_title: str\n",
    "    properties: list[Property] = Field(\n",
    "        ..., description=\"List of properties of the entity\"\n",
    "    )\n",
    "    dependencies: list[int] = Field(\n",
    "        ...,\n",
    "        description=\"List of entity ids that this entity depends  or relies on to resolve it\",\n",
    "    )\n",
    "\n",
    "\n",
    "class DocumentExtraction(BaseModel):\n",
    "    entities: list[Entity] = Field(\n",
    "        ...,\n",
    "        description=\"Body of the answer, each fact should be its seperate object with a body and a list of sources\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample.txt\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "extracted = call_openai(DocumentExtraction, f\"Extract and resolve a list of entities from the following document:\\n\\n{data}\", model=GPT_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<table border='0' cellborder='1' cellspacing='0'><tr><td colspan='2'><b>Agreement Date</b></td></tr><tr><td>Date</td><td>2020-01-01</td></tr></table>>\n",
      "<<table border='0' cellborder='1' cellspacing='0'><tr><td colspan='2'><b>Delivery Date</b></td></tr><tr><td>Period After Agreement Date</td><td>2020-01-31</td></tr></table>>\n",
      "<<table border='0' cellborder='1' cellspacing='0'><tr><td colspan='2'><b>Total Payment</b></td></tr><tr><td>Amount</td><td>$50,000</td></tr></table>>\n",
      "<<table border='0' cellborder='1' cellspacing='0'><tr><td colspan='2'><b>Initial Payment</b></td></tr><tr><td>Amount</td><td>$10,000</td></tr><tr><td>Due Period After Signed Date</td><td>2020-01-08</td></tr></table>>\n",
      "<<table border='0' cellborder='1' cellspacing='0'><tr><td colspan='2'><b>Final Payment Date</b></td></tr><tr><td>Period After Signed Date</td><td>2020-02-15</td></tr></table>>\n",
      "<<table border='0' cellborder='1' cellspacing='0'><tr><td colspan='2'><b>Confidentiality Period End Date</b></td></tr><tr><td>Period After Final Payment Date</td><td>2020-05-15</td></tr></table>>\n",
      "<<table border='0' cellborder='1' cellspacing='0'><tr><td colspan='2'><b>Termination Notice Period</b></td></tr><tr><td>Notice Period</td><td>30-day</td></tr></table>>\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def generate_html_label(entity: Entity) -> str:\n",
    "    rows = [f\"<tr><td>{prop.key}</td><td>{prop.resolved_absolute_value}</td></tr>\" for prop in entity.properties]\n",
    "    table_rows = \"\".join(rows)\n",
    "    return f\"<<table border='0' cellborder='1' cellspacing='0'><tr><td colspan='2'><b>{entity.entity_title}</b></td></tr>{table_rows}</table>>\"\n",
    "\n",
    "def generate_graph(data: DocumentExtraction):\n",
    "    dot = Digraph(comment=\"Entity Graph\", node_attr={\"shape\": \"plaintext\"})\n",
    "\n",
    "    for entity in data.entities:\n",
    "        label = generate_html_label(entity)\n",
    "        dot.node(str(entity.id), label)\n",
    "\n",
    "    for entity in data.entities:\n",
    "        for dep_id in entity.dependencies:\n",
    "            dot.edge(str(entity.id), str(dep_id))\n",
    "\n",
    "    dot.render(\"entity.gv\", view=True)\n",
    "\n",
    "generate_graph(extracted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"subquote_string\": [\n",
      "        \"Wouldn’t the point of Python to be dynamic, rely on duck typing and enable the engineer to create something quickly without barriers? Well, yes, it is.\"\n",
      "      ],\n",
      "      \"entity_title\": \"Python\",\n",
      "      \"properties\": [\n",
      "        {\n",
      "          \"key\": \"importance\",\n",
      "          \"value\": \"dynamic\",\n",
      "          \"resolved_absolute_value\": \"dynamic\"\n",
      "        },\n",
      "        {\n",
      "          \"key\": \"typical_usage\",\n",
      "          \"value\": \"dynamic typing, duck typing\",\n",
      "          \"resolved_absolute_value\": \"dynamic typing, duck typing\"\n",
      "        },\n",
      "        {\n",
      "          \"key\": \"advantage\",\n",
      "          \"value\": \"creation without barriers\",\n",
      "          \"resolved_absolute_value\": \"creation without barriers\"\n",
      "        }\n",
      "      ],\n",
      "      \"dependencies\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"subquote_string\": [\n",
      "        \"gradual typing in Python code (the ability to add type hints to your code little by little over time)\"\n",
      "      ],\n",
      "      \"entity_title\": \"gradual typing\",\n",
      "      \"properties\": [\n",
      "        {\n",
      "          \"key\": \"definition\",\n",
      "          \"value\": \"the ability to add type hints to code gradually\",\n",
      "          \"resolved_absolute_value\": \"the ability to add type hints to code gradually\"\n",
      "        }\n",
      "      ],\n",
      "      \"dependencies\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"subquote_string\": [\n",
      "        \"type hints\"\n",
      "      ],\n",
      "      \"entity_title\": \"type hints\",\n",
      "      \"properties\": [\n",
      "        {\n",
      "          \"key\": \"purpose\",\n",
      "          \"value\": \"increase readability, code quality, and bug detection\",\n",
      "          \"resolved_absolute_value\": \"increase readability, code quality, and bug detection\"\n",
      "        }\n",
      "      ],\n",
      "      \"dependencies\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"subquote_string\": [\n",
      "        \"Allow functions, methods, and classes to work with arguments of any type whilst maintaining the information on the relationships between things, such as arguments and return values.\"\n",
      "      ],\n",
      "      \"entity_title\": \"generics\",\n",
      "      \"properties\": [\n",
      "        {\n",
      "          \"key\": \"purpose\",\n",
      "          \"value\": \"maintain information on relationships between arguments and return values\",\n",
      "          \"resolved_absolute_value\": \"maintain information on relationships between arguments and return values\"\n",
      "        }\n",
      "      ],\n",
      "      \"dependencies\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"subquote_string\": [\n",
      "        \"The aim of generics are to:Allow functions, methods and classes to work with arguments of any type whilst maintaining the information on the relationships between things, such as arguments and return values.\",\n",
      "        \" Better define how types can mix\"\n",
      "      ],\n",
      "      \"entity_title\": \"aim of generics\",\n",
      "      \"properties\": [\n",
      "        {\n",
      "          \"key\": \"purpose\",\n",
      "          \"value\": \"enable functions, methods, and classes to work with any argument type while maintaining relationship information\",\n",
      "          \"resolved_absolute_value\": \"enable functions, methods, and classes to work with any argument type while maintaining relationship information\"\n",
      "        },\n",
      "        {\n",
      "          \"key\": \"additional_purpose\",\n",
      "          \"value\": \"better define type interactions\",\n",
      "          \"resolved_absolute_value\": \"better define type interactions\"\n",
      "        }\n",
      "      ],\n",
      "      \"dependencies\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"subquote_string\": [\n",
      "        \"generic types\"\n",
      "      ],\n",
      "      \"entity_title\": \"generic types\",\n",
      "      \"properties\": [\n",
      "        {\n",
      "          \"key\": \"example\",\n",
      "          \"value\": \"Using generics in the first() function was a small change, but we now better communicate to the reader the relationship between the argument and the return value, and use that information to allow static analysis tools to check our code is correct.\",\n",
      "          \"resolved_absolute_value\": \"Using generics in the first() function was a small change, but we now better communicate to the reader the relationship between the argument and the return value, and use that information to allow static analysis tools to check our code is correct.\"\n",
      "        }\n",
      "      ],\n",
      "      \"dependencies\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": 7,\n",
      "      \"subquote_string\": [\n",
      "        \"type\",\n",
      "        \"T\",\n",
      "        \"U\"\n",
      "      ],\n",
      "      \"entity_title\": \"T and U\",\n",
      "      \"properties\": [\n",
      "        {\n",
      "          \"key\": \"commonly_used_names\",\n",
      "          \"value\": \"T for Type, U for next alphabet letter\",\n",
      "          \"resolved_absolute_value\": \"T for Type, U for next alphabet letter\"\n",
      "        }\n",
      "      ],\n",
      "      \"dependencies\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": 8,\n",
      "      \"subquote_string\": [\n",
      "        \"the upper bound type\"\n",
      "      ],\n",
      "      \"entity_title\": \"upper bound type\",\n",
      "      \"properties\": [\n",
      "        {\n",
      "          \"key\": \"definition\",\n",
      "          \"value\": \"limits the types that can be represented by a generic type in Python\",\n",
      "          \"resolved_absolute_value\": \"limits the types that can be represented by a generic type in Python\"\n",
      "        }\n",
      "      ],\n",
      "      \"dependencies\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": 9,\n",
      "      \"subquote_string\": [\n",
      "        \"Generic Types\",\n",
      "        \"User defined generic types\"\n",
      "      ],\n",
      "      \"entity_title\": \"Generic Types\",\n",
      "      \"properties\": [\n",
      "        {\n",
      "          \"key\": \"usage\",\n",
      "          \"value\": \"used for defining classes that can work with multiple types\",\n",
      "          \"resolved_absolute_value\": \"used for defining classes that can work with multiple types\"\n",
      "        }\n",
      "      ],\n",
      "      \"dependencies\": []\n",
      "    },\n",
      "    {\n",
      "      \"id\": 10,\n",
      "      \"subquote_string\": [\n",
      "        \"User defined generic types\"\n",
      "      ],\n",
      "      \"entity_title\": \"User defined generic types\",\n",
      "      \"properties\": [\n",
      "        {\n",
      "          \"key\": \"example\",\n",
      "          \"value\": \"Generics have allowed us to create a class that can be used with multiple types and then enforce that only arguments of the specified type are sent to the methods of the instance.\",\n",
      "          \"resolved_absolute_value\": \"Generics have allowed us to create a class that can be used with multiple types and then enforce that only arguments of the specified type are sent to the methods of the instance.\"\n",
      "        }\n",
      "      ],\n",
      "      \"dependencies\": []\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(extracted.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Nikolay Mihaylov' age=28\n"
     ]
    }
   ],
   "source": [
    "class UserDetail(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "user = call_openai(\n",
    "    UserDetail,\n",
    "    \"Fillinthedetailesaboutnikolaymihaylovofthatwasburnin1995its2023now\",\n",
    "    # model=GPT_4,\n",
    ")\n",
    "\n",
    "assert isinstance(user, UserDetail)\n",
    "print(user)\n",
    "# print(user.age)\n",
    "# assert user.name == \"Jason\"\n",
    "# assert user.age == 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class Query (BaseModel):\n",
    "    id: int\n",
    "    question: str\n",
    "    dependancies: list[int] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of sub questions that need to be answered before we can ask the question. Use a subquery when anything may be unknown, and we need to ask multiple questions to get the answer. Dependences must only be other queries. Max length is 5. You can use placeholders for unknowns , and the AI will fill them in. Placeholders are in the form of {placeholder_name}.\",\n",
    "    )\n",
    "\n",
    "class QueryPlan(BaseModel):\n",
    "    query_graph: list[Query]\n",
    "\n",
    "\n",
    "\n",
    "plan = call_openai(\n",
    "    QueryPlan,\n",
    "    \"What are the volume of the Earth?\",\n",
    "    model=GPT_4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"query_graph\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"question\": \"What is the volume of the Earth?\",\n",
      "      \"dependancies\": []\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "print(plan.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "class Node(BaseModel):\n",
    "    id: int\n",
    "    label: str = Field(\n",
    "        ...,\n",
    "        description=\"The text that will be displayed on the node. This should be a short phrase that describes the concept needed to answer the question\"\n",
    "    )\n",
    "    color: str\n",
    "class Edge(BaseModel):\n",
    "    source: int\n",
    "    target: int\n",
    "    label: str\n",
    "    color: str = \"black\"\n",
    "class KnowledgeGraph(BaseModel):\n",
    "    nodes: list[Node] = Field(default_factory=list)\n",
    "    edges: list[Edge] = Field(default_factory=list)\n",
    "    def visualize_knowledge_graph(self):\n",
    "        dot = Digraph(comment=\"Knowledge Graph\")\n",
    "        # Add nodes\n",
    "        for node in self.nodes:\n",
    "            dot.node(str(node.id), node.label, color=node.color)\n",
    "        # Add edges\n",
    "        for edge in self.edges:\n",
    "            dot.edge(str(edge.source), str(edge.target), edge.label, color=edge.color)\n",
    "        # Render the graph\n",
    "        dot.render(\"knowledge_graph.gv\", view=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(question: str) -> KnowledgeGraph:\n",
    "    return call_openai(\n",
    "        KnowledgeGraph,\n",
    "        \"Help me understand following by describing as a detailed knowledge graph: {input}\",\n",
    "    )\n",
    "\n",
    "graph = create_graph(\"Who will be the next president of the United States?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"label\": \"Knowledge Graph\",\n",
      "      \"color\": \"blue\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"label\": \"Input\",\n",
      "      \"color\": \"green\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"label\": \"Understanding\",\n",
      "      \"color\": \"yellow\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"label\": \"Description\",\n",
      "      \"color\": \"orange\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"label\": \"Detailed\",\n",
      "      \"color\": \"purple\"\n",
      "    }\n",
      "  ],\n",
      "  \"edges\": [\n",
      "    {\n",
      "      \"source\": 2,\n",
      "      \"target\": 1,\n",
      "      \"label\": \"is part of\",\n",
      "      \"color\": \"black\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": 1,\n",
      "      \"target\": 3,\n",
      "      \"label\": \"leads to\",\n",
      "      \"color\": \"black\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": 3,\n",
      "      \"target\": 4,\n",
      "      \"label\": \"involves\",\n",
      "      \"color\": \"black\"\n",
      "    },\n",
      "    {\n",
      "      \"source\": 4,\n",
      "      \"target\": 5,\n",
      "      \"label\": \"contains\",\n",
      "      \"color\": \"black\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(graph.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.visualize_knowledge_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"correctness\": {\n",
      "    \"description\": \"The text is grammatically correct. It uses appropriate punctuation, correct sentence structure, and proper use of technical terms. The use of parentheses to provide additional information is also correct.\",\n",
      "    \"score\": \"A\"\n",
      "  },\n",
      "  \"politeness\": {\n",
      "    \"description\": \"The text is polite and professional. It provides helpful suggestions and guidance without being condescending or dismissive. The use of 'you might want to' and 'you can' phrases makes the suggestions sound more like friendly advice rather than orders.\",\n",
      "    \"score\": \"A\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model=\"gpt-4-0613\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Provide an alalysis of the text: {text}\"},\n",
    "    ],\n",
    "    functions=[{\"name\": \"set_text_properties\", \"parameters\": schema}],\n",
    "    function_call={\"name\": \"set_text_properties\"},\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.function_call.arguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"correctness\": {\n",
      "    \"description\": \"The text is mostly grammatically correct, but there is one error. The phrase 'I’ve wrote a lot in the issue' should be 'I’ve written a lot in the issue'.\",\n",
      "    \"score\": \"B\"\n",
      "  },\n",
      "  \"politeness\": {\n",
      "    \"description\": \"The text is polite. It uses respectful language and does not contain any offensive or inappropriate words.\",\n",
      "    \"score\": \"A\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.function_call.arguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
